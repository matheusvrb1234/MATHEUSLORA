{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "989d1c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cuda import cudart\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import tensorrt as trt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a077e37",
   "metadata": {},
   "source": [
    "## Generate input and data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee338ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict()\n",
    "batch_size, seq_len, hidden_size = 4, 1, 4096\n",
    "intermediate_size = 11008\n",
    "config['hidden_size'] = hidden_size\n",
    "config['intermediate_size'] = intermediate_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1236912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.ones(batch_size, seq_len, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56ab5db",
   "metadata": {},
   "source": [
    "## torch MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9a74db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiLUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    See Gaussian Error Linear Units (Hendrycks et al., https://arxiv.org/abs/1606.08415) where the SiLU (Sigmoid Linear\n",
    "    Unit) was originally introduced and coined, and see Sigmoid-Weighted Linear Units for Neural Network Function\n",
    "    Approximation in Reinforcement Learbatch_sizeg (Elfwing et al., https://arxiv.org/abs/1702.03118) and Swish: a Self-Gated\n",
    "    Activation Function (Ramachandran et al., https://arxiv.org/abs/1710.05941v1) where the SiLU was experimented with\n",
    "    later.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return nn.functional.silu(input)\n",
    "    \n",
    "    def b_forward(self, input: Tensor) -> Tensor:\n",
    "        return torch.matmul(input.T, nn.functional.sigmoid(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56d861b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.intermediate_size = config['intermediate_size']\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "        self.act_fn = SiLUActivation()\n",
    "        self.init = False\n",
    "\n",
    "    def load(self, dir):\n",
    "        weights = torch.load(dir)\n",
    "        mlp_weights = dict()\n",
    "        for key in weights.keys():\n",
    "            if key.split(\".\")[3] == \"mlp\":\n",
    "                mlp_weights[key[key.find(key.split(\".\")[4]):]] = weights[key]\n",
    "\n",
    "        self.load_state_dict(mlp_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "        return down_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da37bd9f",
   "metadata": {},
   "source": [
    "## Test torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "839f74bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058]],\n",
      "\n",
      "        [[ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058]],\n",
      "\n",
      "        [[ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058]],\n",
      "\n",
      "        [[ 2.0781,  4.5118,  3.0771,  ..., -2.6065, -2.1167, -2.7058]]],\n",
      "       device='cuda:1', grad_fn=<UnsafeViewBackward0>)\n",
      "torch.Size([4, 1, 4096])\n"
     ]
    }
   ],
   "source": [
    "model = LlamaMLP(config)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model.load(\"/home/fuchiang137/.cache/huggingface/hub/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/pytorch_model-00019-of-00033.bin\")\n",
    "model = model.to(device)\n",
    "\n",
    "data_D = data.to(device)\n",
    "# output = model(data)\n",
    "output = model(data_D)\n",
    "\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e79c538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11008, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(model.up_proj.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beeb072",
   "metadata": {},
   "source": [
    "## tensorRT MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be35dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq length is not specified, since it is a dynamic size\n",
    "def trt_create(batch_size, hidden_size, intermediate_size, model):\n",
    "    \n",
    "    logger = trt.Logger(trt.Logger.ERROR)\n",
    "    builder = trt.Builder(logger)\n",
    "\n",
    "    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "    config = builder.create_builder_config()\n",
    "\n",
    "    # input\n",
    "    inputT0 = network.add_input('inputT0', trt.DataType.FLOAT, (batch_size, 1, -1, hidden_size))\n",
    "\n",
    "    # dynamic shape optimization\n",
    "    profile = builder.create_optimization_profile();\n",
    "    profile.set_shape(\"inputT0\", (batch_size, 1, 1, hidden_size), (batch_size, 1, 2, hidden_size), (batch_size, 1, 3, hidden_size))\n",
    "    config.add_optimization_profile(profile)\n",
    "\n",
    "\n",
    "    # self.up_proj(x)\n",
    "    up_proj_weight = model.up_proj.weight.clone().detach().cpu().numpy()\n",
    "    up_proj_layer = network.add_fully_connected(inputT0, model.intermediate_size, up_proj_weight)\n",
    "\n",
    "    # act_fn(self.gate_proj(x))\n",
    "    gate_proj_weight = model.gate_proj.weight.clone().detach().cpu().numpy()\n",
    "    gate_proj_layer = network.add_fully_connected(inputT0, model.intermediate_size, gate_proj_weight)\n",
    "\n",
    "    selu_sigmoid_layer = network.add_activation(gate_proj_layer.get_output(0), type=trt.ActivationType.SIGMOID)\n",
    "    selu_mult_layer = network.add_elementwise(gate_proj_layer.get_output(0), selu_sigmoid_layer.get_output(0), op=trt.ElementWiseOperation.PROD)\n",
    "\n",
    "    # act_fn(self.gate_proj(x)) * self.up_proj(x)\n",
    "    before_down_proj_layer = network.add_elementwise(selu_mult_layer.get_output(0), up_proj_layer.get_output(0), op=trt.ElementWiseOperation.PROD)\n",
    "\n",
    "    down_proj_weight = model.down_proj.weight.clone().detach().cpu().numpy()\n",
    "    down_proj_layer = network.add_fully_connected(before_down_proj_layer.get_output(0), hidden_size, down_proj_weight)\n",
    "\n",
    "    # output\n",
    "    network.mark_output(down_proj_layer.get_output(0))\n",
    "\n",
    "    engineString = builder.build_serialized_network(network, config)\n",
    "    \n",
    "    return engineString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f122f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26430/427192425.py:22: DeprecationWarning: Use add_matrix_multiply instead.\n",
      "  up_proj_layer = network.add_fully_connected(inputT0, model.intermediate_size, up_proj_weight)\n",
      "/tmp/ipykernel_26430/427192425.py:26: DeprecationWarning: Use add_matrix_multiply instead.\n",
      "  gate_proj_layer = network.add_fully_connected(inputT0, model.intermediate_size, gate_proj_weight)\n",
      "/tmp/ipykernel_26430/427192425.py:35: DeprecationWarning: Use add_matrix_multiply instead.\n",
      "  down_proj_layer = network.add_fully_connected(before_down_proj_layer.get_output(0), hidden_size, down_proj_weight)\n"
     ]
    }
   ],
   "source": [
    "trt_engineStr = trt_create(batch_size, hidden_size, intermediate_size, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fbf4b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trt_inference(batch_size, hidden_size, engineString, raw_data, up_proj):\n",
    "#     print(engineString)\n",
    "#     print(\"Runtime\")\n",
    "    logger = trt.Logger(trt.Logger.ERROR)\n",
    "    engine = trt.Runtime(logger).deserialize_cuda_engine(engineString)\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # dynamic shape configure\n",
    "#     print(\"Set input shape\", (batch_size, 1, hidden_size))\n",
    "#     context.set_input_shape(\"inputT0\", (batch_size, 1, hidden_size))\n",
    "#     context.set_binding_shape(0, (batch_size, 1, hidden_size))\n",
    "#     origin_inputshape = context.get_binding_shape(0)\n",
    "\n",
    "#     print(\"Set input shape completed\")\n",
    "\n",
    "    data = np.array(raw_data)\n",
    "\n",
    "    _, stream = cudart.cudaStreamCreate()\n",
    "#     print(\"Reshaping\")\n",
    "\n",
    "    inputH0 = np.ascontiguousarray(data.reshape(-1))\n",
    "    outputH0 = np.empty(context.get_binding_shape(1), dtype=trt.nptype(engine.get_binding_dtype(1)))\n",
    "#     print(\"Reshaped\")\n",
    "\n",
    "    # initialize input and output data\n",
    "    _, inputD0 = cudart.cudaMallocAsync(inputH0.nbytes, stream)\n",
    "    _, outputD0 = cudart.cudaMallocAsync(outputH0.nbytes, stream)\n",
    "\n",
    "    # move input to device\n",
    "    cudart.cudaMemcpyAsync(inputD0, inputH0.ctypes.data, inputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, stream)\n",
    "\n",
    "    # execute\n",
    "#     print(\"execute\")\n",
    "    context.execute_async_v2([int(inputD0), int(outputD0)], stream)\n",
    "\n",
    "    # move output back to host\n",
    "    cudart.cudaMemcpyAsync(outputH0.ctypes.data, outputD0, outputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream)\n",
    "\n",
    "    # wait for everythidden_sizeg\n",
    "    cudart.cudaStreamSynchronize(stream)\n",
    "\n",
    "    cudart.cudaStreamDestroy(stream)\n",
    "    cudart.cudaFree(inputD0)\n",
    "    cudart.cudaFree(outputD0)\n",
    "\n",
    "    return outputH0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5063800b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_trt : (4, 1, 4096)\n",
      "[[[ 2.0781114  4.51184    3.0770564 ... -2.6064956 -2.116741  -2.7058382]]\n",
      "\n",
      " [[ 2.0781114  4.51184    3.0770564 ... -2.6064956 -2.116741  -2.7058382]]\n",
      "\n",
      " [[ 2.0781114  4.51184    3.0770564 ... -2.6064956 -2.116741  -2.7058382]]\n",
      "\n",
      " [[ 2.0781114  4.51184    3.0770564 ... -2.6064956 -2.116741  -2.7058382]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26430/1054781966.py:22: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  outputH0 = np.empty(context.get_binding_shape(1), dtype=trt.nptype(engine.get_binding_dtype(1)))\n",
      "/tmp/ipykernel_26430/1054781966.py:22: DeprecationWarning: Use get_tensor_dtype instead.\n",
      "  outputH0 = np.empty(context.get_binding_shape(1), dtype=trt.nptype(engine.get_binding_dtype(1)))\n"
     ]
    }
   ],
   "source": [
    "up_proj_weight = model.up_proj.weight.clone().detach().cpu().numpy()\n",
    "\n",
    "trt_output = trt_inference(batch_size, hidden_size, trt_engineStr, data, up_proj_weight)\n",
    "\n",
    "trt_output = trt_output.reshape(batch_size, seq_len, hidden_size)\n",
    "print(\"output_trt :\", trt_output.shape)\n",
    "print(trt_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1171496",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ee905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a978551b",
   "metadata": {},
   "source": [
    "### Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "05159fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch memory exe 0.2631836 ms\n"
     ]
    }
   ],
   "source": [
    "torch_start = time.time_ns()\n",
    "\n",
    "output = model(data_D)\n",
    "\n",
    "torch_complete = time.time_ns()\n",
    "\n",
    "print(\"torch memory exe\", (torch_complete - torch_start) / 10e6, \"ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af8294b",
   "metadata": {},
   "source": [
    "### TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008d89c0",
   "metadata": {},
   "source": [
    "### profile CPU/GPU time for tensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4adae348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_trt_inference(batch_size, hidden_size, engineString, raw_data, up_proj):\n",
    "    trt_prep_start = time.time_ns()\n",
    "    \n",
    "    logger = trt.Logger(trt.Logger.ERROR)\n",
    "    engine = trt.Runtime(logger).deserialize_cuda_engine(engineString)\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    trt_prep_complete = time.time_ns()\n",
    "\n",
    "    data = np.array(raw_data)\n",
    "\n",
    "    inputH0 = np.ascontiguousarray(data.reshape(-1))\n",
    "    outputH0 = np.empty(context.get_binding_shape(1), dtype=trt.nptype(engine.get_binding_dtype(1)))\n",
    "\n",
    "    memory_alloc_complete = time.time_ns()\n",
    "\n",
    "    _, stream = cudart.cudaStreamCreate()\n",
    "\n",
    "    # initialize input and output data\n",
    "    _, inputD0 = cudart.cudaMallocAsync(inputH0.nbytes, stream)\n",
    "    _, outputD0 = cudart.cudaMallocAsync(outputH0.nbytes, stream)\n",
    "\n",
    "    # move input to device\n",
    "    cudart.cudaMemcpyAsync(inputD0, inputH0.ctypes.data, inputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, stream)\n",
    "\n",
    "    # execute\n",
    "    context.execute_async_v2([int(inputD0), int(outputD0)], stream)\n",
    "\n",
    "    # move output back to host\n",
    "    cudart.cudaMemcpyAsync(outputH0.ctypes.data, outputD0, outputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream)\n",
    "\n",
    "    # wait for everythidden_sizeg\n",
    "    cudart.cudaStreamSynchronize(stream)\n",
    "\n",
    "    cudart.cudaStreamDestroy(stream)\n",
    "    cudart.cudaFree(inputD0)\n",
    "    cudart.cudaFree(outputD0)\n",
    "\n",
    "    trt_complete = time.time_ns()\n",
    "    \n",
    "    print(\"trt_prep\", (trt_prep_complete - trt_prep_start) / 10e6, \"ms\")\n",
    "    print(\"memory_alloc CPU\", (memory_alloc_complete - trt_prep_complete) / 10e6, \"ms\")\n",
    "    print(\"trt memory alloc & mv & exe\", (trt_complete - memory_alloc_complete) / 10e6, \"ms\")\n",
    "\n",
    "    return outputH0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9d43dbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trt_prep 15.1114614 ms\n",
      "memory_alloc CPU 0.0241049 ms\n",
      "trt memory alloc & mv & exe 0.1599591 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26430/2630322136.py:13: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  outputH0 = np.empty(context.get_binding_shape(1), dtype=trt.nptype(engine.get_binding_dtype(1)))\n",
      "/tmp/ipykernel_26430/2630322136.py:13: DeprecationWarning: Use get_tensor_dtype instead.\n",
      "  outputH0 = np.empty(context.get_binding_shape(1), dtype=trt.nptype(engine.get_binding_dtype(1)))\n"
     ]
    }
   ],
   "source": [
    "trt_output = profile_trt_inference(batch_size, hidden_size, trt_engineStr, data, up_proj_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b9335c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
